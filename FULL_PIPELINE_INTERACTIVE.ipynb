{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76146a7e",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16d00f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: pandas in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: scikit-image in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (0.25.2)\n",
      "Requirement already satisfied: filelock in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-image) (2025.9.9)\n",
      "Requirement already satisfied: packaging>=21 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/newuser/Desktop/Mukesh_AFM/.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install torch torchvision pandas scikit-learn scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539168f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from skimage.filters import threshold_niblack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the current notebook directory and project root\n",
    "notebook_dir = Path(os.getcwd())\n",
    "project_root = notebook_dir  # All files are in the same directory\n",
    "\n",
    "# Add pipeline modules to path\n",
    "pipeline_dir = notebook_dir  # All modules are in this directory\n",
    "sys.path.insert(0, str(pipeline_dir))\n",
    "\n",
    "# Import pipeline modules\n",
    "import importlib.util\n",
    "\n",
    "# Load modules\n",
    "spec1 = importlib.util.spec_from_file_location(\"cnn_inference\", pipeline_dir / \"1.cnn_inference.py\")\n",
    "cnn_inference = importlib.util.module_from_spec(spec1)\n",
    "spec1.loader.exec_module(cnn_inference)\n",
    "\n",
    "spec2 = importlib.util.spec_from_file_location(\"segmentation\", pipeline_dir / \"2.segmentation.py\")\n",
    "segmentation = importlib.util.module_from_spec(spec2)\n",
    "spec2.loader.exec_module(segmentation)\n",
    "\n",
    "spec3 = importlib.util.spec_from_file_location(\"dan_segmentation\", pipeline_dir / \"2.2.dansegmentation.py\")\n",
    "dan_segmentation = importlib.util.module_from_spec(spec3)\n",
    "spec3.loader.exec_module(dan_segmentation)\n",
    "\n",
    "# Import Color Wheel\n",
    "spec4 = importlib.util.spec_from_file_location(\"colorwheel\", pipeline_dir / \"3.colorwheel.py\")\n",
    "colorwheel = importlib.util.module_from_spec(spec4)\n",
    "spec4.loader.exec_module(colorwheel)\n",
    "\n",
    "# Note: voronoi_v7 module is not available in current directory structure\n",
    "# If you need Voronoi analysis, please add voronoi_v7.py to the AFM_Web folder\n",
    "voronoi_v7 = None\n",
    "try:\n",
    "    spec_voronoi = importlib.util.spec_from_file_location(\"voronoi_v7\", pipeline_dir / \"voronoi_v7.py\")\n",
    "    if spec_voronoi and spec_voronoi.loader:\n",
    "        voronoi_v7 = importlib.util.module_from_spec(spec_voronoi)\n",
    "        spec_voronoi.loader.exec_module(voronoi_v7)\n",
    "        print(\"Voronoi module loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Voronoi module not available: {e}\")\n",
    "    print(\"Voronoi analysis will be disabled.\")\n",
    "\n",
    "print(\"All modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851e921",
   "metadata": {},
   "source": [
    "## Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline functions ready\n"
     ]
    }
   ],
   "source": [
    "# Modular pipeline functions\n",
    "from skimage.measure import label, regionprops\n",
    "import skimage\n",
    "from skimage import color\n",
    "\n",
    "def run_cnn_classification_step(image_path):\n",
    "    \"\"\"Step 1: Classify image using CNN (on binarized image) - SILENT\"\"\"\n",
    "    img_path = Path(image_path)\n",
    "    if not img_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "    \n",
    "    # Create temporary binarized image for classification\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    _, binary_img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    temp_binary_path = str(img_path).replace('.png', '_temp_binary.png')\n",
    "    cv2.imwrite(temp_binary_path, binary_img)\n",
    "    \n",
    "    # Load CNN model and classify - model is in the same directory\n",
    "    model_path = pipeline_dir / \"cnn_classifier.pth\"\n",
    "    cnn_model = cnn_inference.load_model(str(model_path))\n",
    "    result = cnn_inference.predict_image(cnn_model, str(temp_binary_path))\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    if Path(temp_binary_path).exists():\n",
    "        Path(temp_binary_path).unlink()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_segmentation_step(image_path, threshold=0.5, denoise=0, sharpen=0, invert=False):\n",
    "    \"\"\"Step 2: U-Net segmentation - SILENT\"\"\"\n",
    "    mask_path = segmentation.segment_image(\n",
    "        image_path=image_path,\n",
    "        model_path=str(pipeline_dir / 'best_quality_unet.pt'),\n",
    "        output_dir='segmentation_output',\n",
    "        threshold=threshold,\n",
    "        denoise=denoise,\n",
    "        sharpen=sharpen,\n",
    "        invert=False\n",
    "    )\n",
    "    \n",
    "    # Optionally invert mask\n",
    "    if invert:\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = 255 - mask\n",
    "        mask_path_inv = mask_path.replace('_mask.png', '_mask_inverted.png')\n",
    "        cv2.imwrite(mask_path_inv, mask)\n",
    "        mask_path = mask_path_inv\n",
    "    \n",
    "    return mask_path\n",
    "\n",
    "def extract_dots_step(mask_path, min_circularity=0.6, max_aspect_ratio=1.8, min_area=15, max_area=400):\n",
    "    \"\"\"Step 3: Extract dots only - SILENT\"\"\"\n",
    "    img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Morphological opening\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # Find connected components\n",
    "    labels_img = label(opened > 0)\n",
    "    props = regionprops(labels_img)\n",
    "    \n",
    "    # Filter by shape and create 5x5 dots\n",
    "    output = np.zeros_like(binary)\n",
    "    stats = {'total': len(props), 'kept': 0}\n",
    "    \n",
    "    for prop in props:\n",
    "        area = prop.area\n",
    "        perimeter = prop.perimeter\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0\n",
    "        aspect = prop.major_axis_length / prop.minor_axis_length if prop.minor_axis_length > 0 else float('inf')\n",
    "        \n",
    "        if (min_area <= area <= max_area and circularity >= min_circularity and aspect <= max_aspect_ratio):\n",
    "            cy, cx = map(int, prop.centroid)\n",
    "            # Create consistent 5x5 dots\n",
    "            y1, y2 = max(cy-2, 0), min(cy+3, output.shape[0])\n",
    "            x1, x2 = max(cx-2, 0), min(cx+3, output.shape[1])\n",
    "            output[y1:y2, x1:x2] = 255\n",
    "            stats['kept'] += 1\n",
    "    \n",
    "    output_path = mask_path.replace('.png', '_DOTS_ONLY.png')\n",
    "    cv2.imwrite(output_path, output)\n",
    "    \n",
    "    return output_path, stats\n",
    "\n",
    "def run_voronoi_step(mask_path, image_size=1.0, threshold_edge=0.025, max_size=1024, auto_detect_features=True):\n",
    "    \"\"\"Step 4: Voronoi analysis - SILENT\n",
    "    \n",
    "    Args:\n",
    "        mask_path: Path to the mask image\n",
    "        image_size: Physical size of image in micrometers\n",
    "        threshold_edge: Edge detection threshold\n",
    "        max_size: Maximum image dimension\n",
    "        auto_detect_features: If True, automatically detect which phase is features (minority)\n",
    "    \"\"\"\n",
    "    if voronoi_v7 is None:\n",
    "        print(\"Voronoi module not available. Please add voronoi_v7.py to the AFM_Web folder.\")\n",
    "        return {'error': 'Voronoi module not available'}\n",
    "    \n",
    "    # Load and process image\n",
    "    im = Image.open(mask_path)\n",
    "    im = im.convert(\"L\")  # Convert to grayscale\n",
    "    img_array = np.array(im)\n",
    "    \n",
    "    # Determine if we need to invert based on which phase is minority\n",
    "    if auto_detect_features:\n",
    "        # Count black vs white pixels\n",
    "        black_pixels = np.sum(img_array < 128)\n",
    "        white_pixels = np.sum(img_array >= 128)\n",
    "        \n",
    "        # If white pixels are minority, they are likely the features\n",
    "        # Voronoi needs black features (0) on white background (1)\n",
    "        if white_pixels < black_pixels:\n",
    "            # White features on black background → invert to get black features on white\n",
    "            img_array = 255 - img_array\n",
    "            print(f\"  Voronoi: Auto-detected WHITE features on BLACK background → Inverting\")\n",
    "        else:\n",
    "            # Black features on white background → keep as is\n",
    "            print(f\"  Voronoi: Auto-detected BLACK features on WHITE background → No inversion\")\n",
    "    else:\n",
    "        # Manual inversion (old behavior)\n",
    "        img_array = 255 - img_array\n",
    "        print(f\"  Voronoi: Manual inversion applied\")\n",
    "    \n",
    "    data = img_array.astype(float) / 255.0\n",
    "    \n",
    "    # Downsample if needed for consistent sizing\n",
    "    if data.shape[0] > max_size or data.shape[1] > max_size:\n",
    "        from skimage.transform import resize\n",
    "        scale = max_size / max(data.shape)\n",
    "        new_shape = (int(data.shape[0] * scale), int(data.shape[1] * scale))\n",
    "        data = resize(data, new_shape, anti_aliasing=True, preserve_range=True)\n",
    "    \n",
    "    image_name = Path(mask_path).stem\n",
    "    output_dir = 'voronoi_outputs'\n",
    "    \n",
    "    # Create output directory structure\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results_folder = os.path.join(output_dir, image_name)\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    \n",
    "    # Run Voronoi analysis\n",
    "    results = voronoi_v7.analyze_image(\n",
    "        image_data=data,\n",
    "        image_name=image_name,\n",
    "        image_size=image_size,\n",
    "        save_image=True,\n",
    "        show_image=False,\n",
    "        save_location=output_dir,\n",
    "        threshold_edge=threshold_edge\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_dan_binarization_step(image_path, method='adaptive', adaptive_method='gaussian',\n",
    "                               block_size=11, C=2, niblack_window=25, k=0.1,\n",
    "                               blur='none', equalize=False, clahe=False):\n",
    "    \"\"\"Step 5a: Dan's binarization - SILENT\"\"\"\n",
    "    binary_path = dan_segmentation.binarize_image(\n",
    "        image_path=image_path,\n",
    "        method=method,\n",
    "        adaptive_method=adaptive_method,\n",
    "        block_size=block_size,\n",
    "        C=C,\n",
    "        niblack_window=niblack_window,\n",
    "        k=k,\n",
    "        blur=blur,\n",
    "        equalize=equalize,\n",
    "        clahe=clahe,\n",
    "        output_dir='dan_binarized'\n",
    "    )\n",
    "    \n",
    "    return binary_path\n",
    "\n",
    "def run_dan_spacing_step(binary_path, image_size_um=2.0, invert=False):\n",
    "    \"\"\"Step 5b: Dan's spacing analysis - SILENT\"\"\"\n",
    "    results = dan_segmentation.analyze_spacing(\n",
    "        image_path=binary_path,\n",
    "        image_size_um=image_size_um,\n",
    "        output_dir='dan_spacing_output',\n",
    "        invert=invert,\n",
    "        save_viz=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_colorwheel_step(mask_path, num_clusters=8):\n",
    "    \"\"\"Step 6: Color wheel analysis for line orientations - Uses MASK not original - SILENT\"\"\"\n",
    "    image_name = Path(mask_path).stem\n",
    "    output_dir = f'colorwheel_output/{image_name}'\n",
    "    \n",
    "    # Run color wheel analysis on the MASK\n",
    "    results = colorwheel.analyze_image(\n",
    "        image_path=mask_path,\n",
    "        output_dir=output_dir,\n",
    "        num_clusters=num_clusters\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Pipeline functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc72834",
   "metadata": {},
   "source": [
    "## Automated Pipeline with Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774456b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73808f7e0d64e048e3405fede799aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>Step 1: Image Input & Classification</h3>'), Text(value='/home/n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== INTERACTIVE STEP-BY-STEP PIPELINE =====\n",
    "\n",
    "# Global state\n",
    "state = {\"step\": 0}\n",
    "\n",
    "# ===== STEP 1: IMAGE INPUT & CLASSIFICATION =====\n",
    "step1_container = widgets.Output()\n",
    "\n",
    "image_path_widget = widgets.Text(\n",
    "    value=str(pipeline_dir / \"Cnn_classifier_test\" / \"dots.png\"),  # Default to test images folder\n",
    "    placeholder=\"Enter image path...\",\n",
    "    description=\"Image Path:\",\n",
    "    style={\"description_width\": \"100px\"},\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "classify_button = widgets.Button(\n",
    "    description=\"Classify Image\",\n",
    "    button_style=\"primary\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "step1_output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_classify(b):\n",
    "    with step1_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Run CNN classification (silent)\n",
    "            cnn_result = run_cnn_classification_step(image_path_widget.value)\n",
    "            state[\"cnn_result\"] = cnn_result\n",
    "            state[\"input_image\"] = image_path_widget.value\n",
    "            state[\"step\"] = 1\n",
    "\n",
    "            predicted_class = cnn_result[\"predicted_class\"]\n",
    "            confidence = cnn_result[\"confidence\"] * 100\n",
    "\n",
    "            # Show classification result with image\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "            img = Image.open(image_path_widget.value)\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.set_title(\n",
    "                f\"Classification: {predicted_class.upper()} ({confidence:.1f}%)\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Enable next step\n",
    "            if predicted_class in [\"dots\", \"lines\", \"mixed\"]:\n",
    "                step2_container.layout.display = \"block\"\n",
    "                step3_container.layout.display = \"none\"\n",
    "            else:  # irregular\n",
    "                step2_container.layout.display = \"none\"\n",
    "                step3_container.layout.display = \"block\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "classify_button.on_click(on_classify)\n",
    "\n",
    "# ===== STEP 2: DOTS/LINES/MIXED PATH =====\n",
    "step2_container = widgets.Output(layout=widgets.Layout(display=\"none\"))\n",
    "\n",
    "# Preprocessing controls\n",
    "seg_denoise = widgets.IntSlider(value=0, min=0, max=30, step=5, description=\"Denoise:\")\n",
    "seg_sharpen = widgets.IntSlider(value=0, min=0, max=10, step=1, description=\"Sharpen:\")\n",
    "\n",
    "# Postprocessing controls\n",
    "seg_threshold = widgets.FloatSlider(\n",
    "    value=0.5, min=0.1, max=0.9, step=0.05, description=\"Threshold:\"\n",
    ")\n",
    "seg_remove_noise = widgets.IntSlider(\n",
    "    value=0, min=0, max=5, step=1, description=\"Remove Noise:\"\n",
    ")\n",
    "seg_invert = widgets.Checkbox(value=False, description=\"Invert Mask\")\n",
    "\n",
    "segment_button = widgets.Button(\n",
    "    description=\"Segment Image\",\n",
    "    button_style=\"info\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "step2_output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_segment(b):\n",
    "    with step2_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Run segmentation (silent)\n",
    "            mask_path = run_segmentation_step(\n",
    "                state[\"input_image\"],\n",
    "                threshold=seg_threshold.value,\n",
    "                denoise=seg_denoise.value,\n",
    "                sharpen=seg_sharpen.value,\n",
    "                invert=seg_invert.value,\n",
    "            )\n",
    "\n",
    "            # Apply noise removal if needed\n",
    "            if seg_remove_noise.value > 0:\n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                kernel = cv2.getStructuringElement(\n",
    "                    cv2.MORPH_ELLIPSE, (seg_remove_noise.value, seg_remove_noise.value)\n",
    "                )\n",
    "                mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "                cv2.imwrite(mask_path, mask)\n",
    "\n",
    "            state[\"segmentation_mask\"] = mask_path\n",
    "            state[\"step\"] = 2\n",
    "\n",
    "            # Show 2-panel visualization\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "            # Original\n",
    "            orig = Image.open(state[\"input_image\"])\n",
    "            axes[0].imshow(orig, cmap=\"gray\")\n",
    "            axes[0].set_title(\"Original Image\", fontsize=12, fontweight=\"bold\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            # Segmented\n",
    "            seg_img = Image.open(mask_path)\n",
    "            axes[1].imshow(seg_img, cmap=\"gray\")\n",
    "            axes[1].set_title(\"U-Net Segmentation\", fontsize=12, fontweight=\"bold\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Route to next step based on classification\n",
    "            predicted_class = state[\"cnn_result\"][\"predicted_class\"]\n",
    "\n",
    "            if predicted_class == \"dots\":\n",
    "                # Show extraction and voronoi only\n",
    "                dot_extract_container.layout.display = \"block\"\n",
    "                voronoi_container.layout.display = \"none\"  # Will show after extraction\n",
    "                colorwheel_container.layout.display = \"none\"\n",
    "            elif predicted_class == \"lines\":\n",
    "                # Show color wheel only\n",
    "                dot_extract_container.layout.display = \"none\"\n",
    "                voronoi_container.layout.display = \"none\"\n",
    "                colorwheel_container.layout.display = \"block\"\n",
    "            elif predicted_class == \"mixed\":\n",
    "                # Show extraction and both analysis options\n",
    "                dot_extract_container.layout.display = \"block\"\n",
    "                voronoi_container.layout.display = \"none\"  # Will show after extraction\n",
    "                colorwheel_container.layout.display = \"block\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "segment_button.on_click(on_segment)\n",
    "\n",
    "# Dot extraction controls\n",
    "dot_extract_container = widgets.VBox(layout=widgets.Layout(display=\"none\"))\n",
    "\n",
    "dot_min_area = widgets.IntSlider(\n",
    "    value=4, min=1, max=50, step=1, description=\"Min Area:\"\n",
    ")\n",
    "dot_max_area = widgets.IntSlider(\n",
    "    value=200, min=50, max=500, step=10, description=\"Max Area:\"\n",
    ")\n",
    "dot_circularity = widgets.FloatSlider(\n",
    "    value=0.7, min=0.1, max=1.0, step=0.05, description=\"Min Circular:\"\n",
    ")\n",
    "dot_aspect_ratio = widgets.FloatSlider(\n",
    "    value=2.5, min=1.0, max=5.0, step=0.1, description=\"Max Aspect:\"\n",
    ")\n",
    "\n",
    "extract_dots_button = widgets.Button(\n",
    "    description=\"Extract Dots\",\n",
    "    button_style=\"info\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "step2_dot_output = widgets.Output()\n",
    "\n",
    "\n",
    "# def on_extract_dots(b):\n",
    "#     with step2_dot_output:\n",
    "#         clear_output(wait=True)\n",
    "#         try:\n",
    "#             # Extract dots (silent)\n",
    "#             dot_stats = extract_dots_step(\n",
    "#                 state[\"segmentation_mask\"],\n",
    "#                 min_area=dot_min_area.value,\n",
    "#                 max_area=dot_max_area.value,\n",
    "#                 min_circularity=dot_circularity.value,\n",
    "#                 max_aspect_ratio=dot_aspect_ratio.value,\n",
    "#             )\n",
    "#             state[\"dot_stats\"] = dot_stats\n",
    "\n",
    "#             # Display results\n",
    "#             print(f\"Extracted {dot_stats['kept']} dots\")\n",
    "#             print(f\"(Rejected {dot_stats['rejected']} regions)\")\n",
    "\n",
    "#             # Enable voronoi analysis (always available after dot extraction)\n",
    "#             voronoi_container.layout.display = \"block\"\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def on_extract_dots(b):\n",
    "    with step2_dot_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Extract dots (silent)\n",
    "            dots_path, dot_stats = extract_dots_step(\n",
    "                state[\"segmentation_mask\"],\n",
    "                min_circularity=dot_circularity.value,\n",
    "                max_aspect_ratio=dot_aspect_ratio.value,\n",
    "                min_area=dot_min_area.value,\n",
    "                max_area=dot_max_area.value,\n",
    "            )\n",
    "            state[\"dots_mask\"] = dots_path\n",
    "            state[\"dot_stats\"] = dot_stats\n",
    "            state[\"step\"] = 3\n",
    "\n",
    "            # Show before/after\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "            mask_before = cv2.imread(state[\"segmentation_mask\"], cv2.IMREAD_GRAYSCALE)\n",
    "            axes[0].imshow(mask_before, cmap=\"gray\")\n",
    "            axes[0].set_title(f\"Before Extraction\", fontsize=12, fontweight=\"bold\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            mask_after = cv2.imread(dots_path, cv2.IMREAD_GRAYSCALE)\n",
    "            axes[1].imshow(mask_after, cmap=\"gray\")\n",
    "            axes[1].set_title(\n",
    "                f'Extracted Dots ({dot_stats[\"kept\"]} features)',\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            if dot_stats[\"kept\"] >= 4:\n",
    "                # Enable Voronoi controls\n",
    "                voronoi_container.layout.display = \"block\"\n",
    "            else:\n",
    "                print(f\"Few dots {dot_stats['kept']} dots found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "extract_dots_button.on_click(on_extract_dots)\n",
    "\n",
    "dot_extract_container.children = [\n",
    "    widgets.HTML(\"<h4>Dot Extraction Parameters</h4>\"),\n",
    "    dot_min_area,\n",
    "    dot_max_area,\n",
    "    dot_circularity,\n",
    "    dot_aspect_ratio,\n",
    "    extract_dots_button,\n",
    "    step2_dot_output,\n",
    "]\n",
    "\n",
    "# Voronoi controls\n",
    "voronoi_container = widgets.VBox(layout=widgets.Layout(display=\"none\"))\n",
    "\n",
    "voronoi_image_size = widgets.FloatText(value=1.0, description=\"Image Size (μm):\")\n",
    "voronoi_threshold_edge = widgets.FloatSlider(\n",
    "    value=0.025,\n",
    "    min=0.01,\n",
    "    max=0.1,\n",
    "    step=0.005,\n",
    "    description=\"Edge Threshold:\",\n",
    "    readout_format=\".3f\",\n",
    ")\n",
    "voronoi_max_size = widgets.IntSlider(\n",
    "    value=1024, min=512, max=2048, step=256, description=\"Max Size (px):\"\n",
    ")\n",
    "voronoi_auto_detect = widgets.Checkbox(\n",
    "    value=True, \n",
    "    description=\"Auto-detect features\",\n",
    "    tooltip=\"Automatically detect which phase is features (minority pixels)\"\n",
    ")\n",
    "\n",
    "run_voronoi_button = widgets.Button(\n",
    "    description=\"Run Voronoi Analysis\",\n",
    "    button_style=\"success\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "step2_voronoi_output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_run_voronoi(b):\n",
    "    with step2_voronoi_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Run voronoi analysis (silent) - uses extracted dots mask\n",
    "            voronoi_results = run_voronoi_step(\n",
    "                state[\"dots_mask\"],\n",
    "                image_size=voronoi_image_size.value,\n",
    "                threshold_edge=voronoi_threshold_edge.value,\n",
    "                max_size=voronoi_max_size.value,\n",
    "                auto_detect_features=voronoi_auto_detect.value,\n",
    "            )\n",
    "            state[\"voronoi_results\"] = voronoi_results\n",
    "\n",
    "            # Show Voronoi visualization\n",
    "            image_name = Path(state[\"dots_mask\"]).stem\n",
    "            overlay_path = (\n",
    "                Path(\"voronoi_outputs\")\n",
    "                / image_name\n",
    "                / f\"{image_name}_voronoi_overlay.png\"\n",
    "            )\n",
    "            # Display overlay image if it exists\n",
    "            # output_dir = Path(f\"voronoi_output/{Path(state['segmentation_mask']).stem}\")\n",
    "            # overlay_path = output_dir / \"overlay_voronoi.png\"\n",
    "\n",
    "            if overlay_path.exists():\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "                img = Image.open(overlay_path)\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(\"Voronoi Tessellation\", fontsize=14, fontweight=\"bold\")\n",
    "                ax.axis(\"off\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            # Display key results\n",
    "            print(\"=\" * 50)\n",
    "            print(\"VORONOI ANALYSIS RESULTS\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Periodicity: {voronoi_results.get('periodicity', 'N/A'):.2f} nm\")\n",
    "            print(f\"Block Ratio: {voronoi_results.get('block_ratio', 'N/A'):.4f}\")\n",
    "            print(f\"Mean Morphology: {voronoi_results.get('mean_morph', 'N/A'):.2f}\")\n",
    "            print(f\"Number of Dots: {state['dot_stats']['kept']}\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "run_voronoi_button.on_click(on_run_voronoi)\n",
    "\n",
    "voronoi_container.children = [\n",
    "    widgets.HTML(\"<h4>Voronoi Analysis Parameters</h4>\"),\n",
    "    voronoi_image_size,\n",
    "    voronoi_threshold_edge,\n",
    "    voronoi_max_size,\n",
    "    voronoi_auto_detect,\n",
    "    run_voronoi_button,\n",
    "    step2_voronoi_output,\n",
    "]\n",
    "\n",
    "# Color Wheel controls (for lines/mixed features)\n",
    "colorwheel_container = widgets.VBox(layout=widgets.Layout(display=\"none\"))\n",
    "\n",
    "colorwheel_num_clusters = widgets.IntSlider(\n",
    "    value=8, min=2, max=12, step=1, description=\"Num Clusters:\"\n",
    ")\n",
    "colorwheel_invert = widgets.Checkbox(value=True, description=\"Invert Image\")\n",
    "\n",
    "run_colorwheel_button = widgets.Button(\n",
    "    description=\"Run Color Wheel\",\n",
    "    button_style=\"success\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "step2_colorwheel_output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_run_colorwheel(b):\n",
    "    with step2_colorwheel_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Color wheel works on SEGMENTATION MASK (after U-Net)\n",
    "            if \"segmentation_mask\" not in state:\n",
    "                print(\"No segmentation mask available. Please run segmentation first.\")\n",
    "                return\n",
    "            \n",
    "            # Read segmentation mask\n",
    "            mask_img = cv2.imread(state[\"segmentation_mask\"], cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Apply invert if requested\n",
    "            if colorwheel_invert.value:\n",
    "                mask_img = 255 - mask_img\n",
    "            \n",
    "            # Show the mask being sent to color wheel\n",
    "            # Hide it for now to reduce output clutter\n",
    "            # fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "            # ax.imshow(mask_img, cmap='gray')\n",
    "            # ax.set_title(f\"Mask Input to Color Wheel {'(Inverted)' if colorwheel_invert.value else ''}\", \n",
    "            #             fontsize=12, fontweight=\"bold\")\n",
    "            # ax.axis('off')\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "            \n",
    "            # Save temporary mask for color wheel processing\n",
    "            temp_dir = Path('colorwheel_temp')\n",
    "            temp_dir.mkdir(exist_ok=True)\n",
    "            temp_img_path = temp_dir / f\"{Path(state['segmentation_mask']).stem}_colorwheel_input.png\"\n",
    "            cv2.imwrite(str(temp_img_path), mask_img)\n",
    "            \n",
    "            input_mask = str(temp_img_path)\n",
    "            \n",
    "            # Run color wheel analysis (silent)\n",
    "            colorwheel_results = run_colorwheel_step(\n",
    "                input_mask,\n",
    "                num_clusters=colorwheel_num_clusters.value\n",
    "            )\n",
    "            state[\"colorwheel_results\"] = colorwheel_results\n",
    "\n",
    "            # Collect all grain mask images\n",
    "            grain_masks = colorwheel_results.get(\"grain_masks\", [])\n",
    "            output_dir = Path(colorwheel_results[\"output_directory\"])\n",
    "            \n",
    "            # Find all Mask_*.tiff files\n",
    "            mask_files = sorted(output_dir.glob(\"Mask_*.tiff\"))\n",
    "            \n",
    "            if not mask_files:\n",
    "                print(\"No grain masks generated\")\n",
    "                return\n",
    "            \n",
    "            # Display results in 2x2 grid (or adjust based on number of masks)\n",
    "            num_masks = len(mask_files)\n",
    "            if num_masks == 0:\n",
    "                print(\"No valid masks found\")\n",
    "                return\n",
    "            \n",
    "            # Create grid layout (2x2 for 4 masks, adjust as needed)\n",
    "            ncols = 2\n",
    "            nrows = (num_masks + 1) // 2\n",
    "            \n",
    "            fig, axes = plt.subplots(nrows, ncols, figsize=(12, 6 * nrows))\n",
    "            \n",
    "            # Flatten axes for easy iteration\n",
    "            if nrows == 1 and ncols == 1:\n",
    "                axes = [axes]\n",
    "            elif nrows == 1 or ncols == 1:\n",
    "                axes = axes.flatten()\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for idx, mask_file in enumerate(mask_files):\n",
    "                if idx < len(axes):\n",
    "                    img = Image.open(mask_file)\n",
    "                    axes[idx].imshow(img)\n",
    "                    axes[idx].set_title(f\"Orientation Mask {idx}\", fontsize=12, fontweight=\"bold\")\n",
    "                    axes[idx].axis(\"off\")\n",
    "            \n",
    "            # Hide unused subplots\n",
    "            for idx in range(len(mask_files), len(axes)):\n",
    "                axes[idx].axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Display key results\n",
    "            print(\"=\" * 50)\n",
    "            print(\"COLOR WHEEL ANALYSIS RESULTS\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Number of Clusters: {colorwheel_results.get('num_clusters', 'N/A')}\")\n",
    "            print(f\"Output Directory: {colorwheel_results.get('output_directory', 'N/A')}\")\n",
    "            print(f\"Number of Grain Masks: {len(mask_files)}\")\n",
    "            print(f\"GPU Accelerated: {colorwheel_results.get('gpu_accelerated', False)}\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "\n",
    "run_colorwheel_button.on_click(on_run_colorwheel)\n",
    "\n",
    "colorwheel_container.children = [\n",
    "    widgets.HTML(\"<h4>Color Wheel Analysis (Lines/Mixed)</h4>\"),\n",
    "    colorwheel_num_clusters,\n",
    "    run_colorwheel_button,\n",
    "    step2_colorwheel_output,\n",
    "]\n",
    "\n",
    "# ===== STEP 3: IRREGULAR PATH =====\n",
    "step3_container = widgets.Output(layout=widgets.Layout(display=\"none\"))\n",
    "\n",
    "# Dan's binarization method widgets\n",
    "common_layout = widgets.Layout(width=\"320px\")\n",
    "\n",
    "use_adaptive = widgets.Checkbox(value=True, description=\"Use Adaptive\")\n",
    "use_niblack = widgets.Checkbox(value=True, description=\"Use Niblack\")\n",
    "\n",
    "combine_method = widgets.Dropdown(\n",
    "    options=[\"adaptive\", \"niblack\", \"AND\", \"OR\", \"weighted\"],\n",
    "    value=\"OR\",\n",
    "    description=\"Combine\"\n",
    ")\n",
    "alpha_weight = widgets.FloatSlider(\n",
    "    value=0.5, min=0.0, max=1.0, step=0.05, description=\"Alpha (weighted only)\"\n",
    ")\n",
    "\n",
    "niblack_k = widgets.FloatSlider(\n",
    "    value=0.1, min=-1.0, max=1.0, step=0.05, description=\"k\"\n",
    ")\n",
    "niblack_window = widgets.IntSlider(\n",
    "    value=25, min=3, max=111, step=2, description=\"Block size\"\n",
    ")\n",
    "\n",
    "adaptive_method = widgets.ToggleButtons(\n",
    "    options={\"MEAN\": 0, \"GAUSSIAN\": 1},\n",
    "    description=\"Adaptive Method\"\n",
    ")\n",
    "adaptive_block_size = widgets.IntSlider(\n",
    "    value=11, min=3, max=101, step=2, description=\"Block size\"\n",
    ")\n",
    "adaptive_C = widgets.IntSlider(\n",
    "    value=2, min=-50, max=30, step=1, description=\"C\"\n",
    ")\n",
    "\n",
    "blur_method = widgets.Dropdown(\n",
    "    options=[\"none\", \"gaussian\", \"median\", \"bilateral\"],\n",
    "    value=\"none\",\n",
    "    description=\"Preprocess Blur\"\n",
    ")\n",
    "blur_ksize = widgets.IntSlider(\n",
    "    value=5, min=3, max=21, step=2, description=\"Blur kernel size\"\n",
    ")\n",
    "dan_equalize = widgets.Checkbox(value=False, description=\"Equalize\")\n",
    "dan_clahe = widgets.Checkbox(value=False, description=\"CLAHE\")\n",
    "\n",
    "# === Conditional Display Logic ===\n",
    "def toggle_niblack_widgets(change):\n",
    "    enabled = change['new']\n",
    "    niblack_k.disabled = not enabled\n",
    "    niblack_window.disabled = not enabled\n",
    "\n",
    "def toggle_adaptive_widgets(change):\n",
    "    enabled = change['new']\n",
    "    adaptive_method.disabled = not enabled\n",
    "    adaptive_block_size.disabled = not enabled\n",
    "    adaptive_C.disabled = not enabled\n",
    "\n",
    "def toggle_alpha_slider(change):\n",
    "    alpha_weight.disabled = (change['new'] != \"weighted\")\n",
    "\n",
    "use_niblack.observe(toggle_niblack_widgets, names='value')\n",
    "use_adaptive.observe(toggle_adaptive_widgets, names='value')\n",
    "combine_method.observe(toggle_alpha_slider, names='value')\n",
    "\n",
    "# Placeholders for layout spacing\n",
    "niblack_placeholder = widgets.HTML(value=\"\", layout=widgets.Layout(height=\"0px\"))\n",
    "adaptive_placeholder = widgets.HTML(value=\"\", layout=widgets.Layout(height=\"0px\"))\n",
    "preprocess_placeholder = widgets.HTML(value=\"\", layout=widgets.Layout(height=\"0px\"))\n",
    "\n",
    "# Initialize display states\n",
    "toggle_niblack_widgets({'new': use_niblack.value})\n",
    "toggle_adaptive_widgets({'new': use_adaptive.value})\n",
    "toggle_alpha_slider({'new': combine_method.value})\n",
    "\n",
    "# Apply common layout to all widgets\n",
    "for w in [\n",
    "    use_adaptive, use_niblack, combine_method, alpha_weight,\n",
    "    niblack_k, niblack_window,\n",
    "    adaptive_method, adaptive_block_size, adaptive_C,\n",
    "    blur_method, blur_ksize,\n",
    "    dan_equalize, dan_clahe\n",
    "]:\n",
    "    w.layout = common_layout\n",
    "    if hasattr(w, 'style'):\n",
    "        w.style.description_width = 'initial'\n",
    "\n",
    "# Interactive preview function (updates automatically as sliders change)\n",
    "def interactive_preview_binary(\n",
    "    use_adapt,\n",
    "    use_nib,\n",
    "    comb_method,\n",
    "    alpha_w,\n",
    "    nib_k,\n",
    "    nib_win,\n",
    "    adapt_method,\n",
    "    adapt_block,\n",
    "    adapt_c,\n",
    "    blur_m,\n",
    "    blur_k,\n",
    "    equalize,\n",
    "    clahe,\n",
    "):\n",
    "    \"\"\"Shows live preview of binarization with current parameters.\"\"\"\n",
    "    if \"input_image\" not in state:\n",
    "        print(\"No image loaded. Please classify an image first.\")\n",
    "        return\n",
    "\n",
    "    img = cv2.imread(state[\"input_image\"], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Preprocess\n",
    "    if blur_m != \"none\":\n",
    "        if blur_m == \"gaussian\":\n",
    "            img = cv2.GaussianBlur(img, (blur_k, blur_k), 0)\n",
    "        elif blur_m == \"median\":\n",
    "            img = cv2.medianBlur(img, blur_k)\n",
    "        elif blur_m == \"bilateral\":\n",
    "            img = cv2.bilateralFilter(img, blur_k, 75, 75)\n",
    "\n",
    "    if equalize:\n",
    "        img = cv2.equalizeHist(img)\n",
    "\n",
    "    if clahe:\n",
    "        clahe_obj = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img = clahe_obj.apply(img)\n",
    "\n",
    "    # Binarize - match Dan's implementation\n",
    "    thresh_adaptive, thresh_niblack = None, None\n",
    "    \n",
    "    if use_adapt:\n",
    "        thresh_adaptive = cv2.adaptiveThreshold(\n",
    "            img, 255, adapt_method, cv2.THRESH_BINARY_INV, adapt_block, adapt_c\n",
    "        )\n",
    "    \n",
    "    if use_nib:\n",
    "        t_niblack = threshold_niblack(img, window_size=nib_win, k=nib_k)\n",
    "        thresh_niblack = (img < t_niblack).astype(np.uint8) * 255\n",
    "    \n",
    "    # Combine or select\n",
    "    final = None\n",
    "    if use_adapt and not use_nib:\n",
    "        final = thresh_adaptive\n",
    "    elif use_nib and not use_adapt:\n",
    "        final = thresh_niblack\n",
    "    elif use_adapt and use_nib:\n",
    "        if comb_method == \"AND\":\n",
    "            final = cv2.bitwise_and(thresh_adaptive, thresh_niblack)\n",
    "        elif comb_method == \"OR\":\n",
    "            final = cv2.bitwise_or(thresh_adaptive, thresh_niblack)\n",
    "        elif comb_method == \"adaptive\":\n",
    "            final = thresh_adaptive\n",
    "        elif comb_method == \"niblack\":\n",
    "            final = thresh_niblack\n",
    "        elif comb_method == \"weighted\":\n",
    "            blend = (\n",
    "                alpha_w * thresh_adaptive.astype(np.float32) +\n",
    "                (1 - alpha_w) * thresh_niblack.astype(np.float32)\n",
    "            )\n",
    "            final = (blend > 127).astype(np.uint8) * 255\n",
    "    \n",
    "    if final is None:\n",
    "        print(\"Must enable at least one method!\")\n",
    "        return\n",
    "\n",
    "    # Store for saving\n",
    "    state[\"preview_binary\"] = final\n",
    "\n",
    "    # Display 3-panel visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Create overlay with magenta/pink color\n",
    "    mask = final == 255\n",
    "    overlay = np.zeros((*final.shape, 4), dtype=np.float32)\n",
    "    overlay[mask] = [0.988, 0.102, 0.973, 0.75]  # Magenta with transparency\n",
    "\n",
    "    axes[0].imshow(img, cmap=\"gray\")\n",
    "    axes[0].set_title(\"Original Gray\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(cv2.imread(state[\"input_image\"], cv2.IMREAD_GRAYSCALE), cmap=\"gray\")\n",
    "    axes[1].imshow(overlay)\n",
    "    axes[1].set_title(\"Processed + Overlay\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(cv2.imread(state[\"input_image\"], cv2.IMREAD_GRAYSCALE), cmap=\"gray\")\n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(\"Original + Overlay\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_preview = widgets.interactive(\n",
    "    interactive_preview_binary,\n",
    "    use_adapt=use_adaptive,\n",
    "    use_nib=use_niblack,\n",
    "    comb_method=combine_method,\n",
    "    alpha_w=alpha_weight,\n",
    "    nib_k=niblack_k,\n",
    "    nib_win=niblack_window,\n",
    "    adapt_method=adaptive_method,\n",
    "    adapt_block=adaptive_block_size,\n",
    "    adapt_c=adaptive_C,\n",
    "    blur_m=blur_method,\n",
    "    blur_k=blur_ksize,\n",
    "    equalize=dan_equalize,\n",
    "    clahe=dan_clahe,\n",
    ")\n",
    "\n",
    "step3_output = widgets.Output()\n",
    "\n",
    "# Final binarization button\n",
    "def on_binarize(b):\n",
    "    with step3_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Check if preview exists\n",
    "            if \"preview_binary\" not in state:\n",
    "                print(\"Please preview binarization first!\")\n",
    "                return\n",
    "\n",
    "            # Save the binarized image\n",
    "            output_dir = \"dan_binarized\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            filename = Path(state[\"input_image\"]).stem\n",
    "            binary_path = os.path.join(output_dir, f\"{filename}_binary.png\")\n",
    "            cv2.imwrite(binary_path, state[\"preview_binary\"])\n",
    "\n",
    "            state[\"dan_binary\"] = binary_path\n",
    "\n",
    "            print(f\"Binarized image saved: {binary_path}\")\n",
    "\n",
    "            # Enable spacing analysis\n",
    "            spacing_container.layout.display = \"block\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Final binarization button\n",
    "binarize_button = widgets.Button(\n",
    "    description=\"Save Binarization\",\n",
    "    button_style=\"success\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "binarize_button.on_click(on_binarize)\n",
    "\n",
    "# Spacing analysis controls\n",
    "spacing_container = widgets.VBox(layout=widgets.Layout(display=\"none\"))\n",
    "\n",
    "spacing_image_size = widgets.FloatText(value=2.0, description=\"Image Size (μm):\")\n",
    "spacing_invert = widgets.Checkbox(value=False, description=\"Invert Features\")\n",
    "\n",
    "run_spacing_button = widgets.Button(\n",
    "    description=\"Analyze Spacing\",\n",
    "    button_style=\"warning\",\n",
    "    layout=widgets.Layout(width=\"200px\", height=\"40px\"),\n",
    ")\n",
    "\n",
    "step3_spacing_output = widgets.Output()\n",
    "\n",
    "\n",
    "def on_run_spacing(b):\n",
    "    with step3_spacing_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            # Determine which binary to use\n",
    "            if \"dan_binary\" in state:\n",
    "                binary_path = state[\"dan_binary\"]\n",
    "            elif \"segmentation_mask\" in state:\n",
    "                binary_path = state[\"segmentation_mask\"]\n",
    "            else:\n",
    "                print(\"No binary mask available!\")\n",
    "                return\n",
    "\n",
    "            # Run spacing analysis (silent)\n",
    "            spacing_results = run_dan_spacing_step(\n",
    "                binary_path,\n",
    "                image_size_um=spacing_image_size.value,\n",
    "                invert=spacing_invert.value,\n",
    "            )\n",
    "            state[\"spacing_results\"] = spacing_results\n",
    "\n",
    "            # Display skeleton and overlay images (match actual output directory)\n",
    "            filename_stem = Path(binary_path).stem\n",
    "            \n",
    "            # Check for histogram and overlay files\n",
    "            histogram_path = Path(f\"dan_spacing_output/{filename_stem}_spacing_histogram.png\")\n",
    "            overlay_path = Path(f\"dan_spacing_output/{filename_stem}_voronoi_overlay.png\")\n",
    "\n",
    "            if histogram_path.exists() and overlay_path.exists():\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "                hist_img = Image.open(histogram_path)\n",
    "                axes[0].imshow(hist_img)\n",
    "                axes[0].set_title(\"Spacing Histogram\", fontsize=12, fontweight=\"bold\")\n",
    "                axes[0].axis(\"off\")\n",
    "\n",
    "                overlay_img = Image.open(overlay_path)\n",
    "                axes[1].imshow(overlay_img)\n",
    "                axes[1].set_title(\n",
    "                    \"Voronoi Skeleton Overlay\",\n",
    "                    fontsize=12,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "                axes[1].axis(\"off\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            # Display results\n",
    "            print(\"=\" * 50)\n",
    "            print(\"SPACING ANALYSIS RESULTS\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Mean Spacing: {spacing_results['mean_spacing_nm']:.2f} nm\")\n",
    "            print(f\"Median Spacing: {spacing_results['median_spacing_nm']:.2f} nm\")\n",
    "            print(f\"Std Deviation: {spacing_results['std_spacing_nm']:.2f} nm\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "run_spacing_button.on_click(on_run_spacing)\n",
    "\n",
    "spacing_container.children = [\n",
    "    widgets.HTML(\"<h4>Spacing Analysis Parameters</h4>\"),\n",
    "    spacing_image_size,\n",
    "    spacing_invert,\n",
    "    run_spacing_button,\n",
    "    step3_spacing_output,\n",
    "]\n",
    "\n",
    "# ===== MAIN UI LAYOUT =====\n",
    "\n",
    "# Build step 2 content (will be hidden initially)\n",
    "step2_content = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\"<h3>Step 2: Segmentation & Analysis</h3>\"),\n",
    "        widgets.HTML(\"<h4>U-Net Segmentation Parameters</h4>\"),\n",
    "        seg_denoise,\n",
    "        seg_sharpen,\n",
    "        seg_threshold,\n",
    "        seg_remove_noise,\n",
    "        seg_invert,\n",
    "        segment_button,\n",
    "        step2_output,\n",
    "        dot_extract_container,\n",
    "        voronoi_container,\n",
    "        colorwheel_container,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build step 3 content (will be hidden initially) - 3-column layout like Dan's\n",
    "ui_left_dan = widgets.VBox([\n",
    "    use_adaptive,\n",
    "    use_niblack,\n",
    "    combine_method,\n",
    "    alpha_weight,\n",
    "    niblack_placeholder,\n",
    "    niblack_k,\n",
    "    niblack_window\n",
    "], layout=widgets.Layout(padding=\"10px\"))\n",
    "\n",
    "ui_middle_dan = widgets.VBox([\n",
    "    adaptive_placeholder,\n",
    "    adaptive_method,\n",
    "    adaptive_block_size,\n",
    "    adaptive_C\n",
    "], layout=widgets.Layout(padding=\"10px\"))\n",
    "\n",
    "ui_right_dan = widgets.VBox([\n",
    "    preprocess_placeholder,\n",
    "    blur_method,\n",
    "    blur_ksize,\n",
    "    dan_equalize,\n",
    "    dan_clahe\n",
    "], layout=widgets.Layout(padding=\"10px\"))\n",
    "\n",
    "step3_content = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\"<h3>Step 3: Dan's Binarization & Spacing Analysis</h3>\"),\n",
    "        widgets.HTML(\"<h4>Binarization Parameters (Interactive Preview)</h4>\"),\n",
    "        widgets.HBox([ui_left_dan, ui_middle_dan, ui_right_dan], \n",
    "                     layout=widgets.Layout(justify_content='space-between')),\n",
    "        interactive_preview.children[-1],  # Only show the output, not the widget controls\n",
    "        binarize_button,\n",
    "        step3_output,\n",
    "        spacing_container,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Main layout with 3 columns\n",
    "ui_left = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\"<h3>Step 1: Image Input & Classification</h3>\"),\n",
    "        image_path_widget,\n",
    "        classify_button,\n",
    "        step1_output,\n",
    "    ]\n",
    ")\n",
    "\n",
    "ui_middle = widgets.VBox([step2_container])\n",
    "ui_right = widgets.VBox([step3_container])\n",
    "\n",
    "# Display step2/step3 content inside their containers\n",
    "with step2_container:\n",
    "    display(step2_content)\n",
    "\n",
    "with step3_container:\n",
    "    display(step3_content)\n",
    "\n",
    "# Main UI\n",
    "ui = widgets.VBox([ui_left, ui_middle, ui_right])\n",
    "display(ui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
